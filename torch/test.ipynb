{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm): LSTM(1, 100)\n",
       "  (linear): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modelo import LSTM\n",
    "from data import *\n",
    "modelo=LSTM()\n",
    "modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>time</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-12-29 08:00:00</td>\n",
       "      <td>16556.37</td>\n",
       "      <td>16630.00</td>\n",
       "      <td>16517.54</td>\n",
       "      <td>16598.70</td>\n",
       "      <td>28212.71787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-12-29 12:00:00</td>\n",
       "      <td>16598.70</td>\n",
       "      <td>16664.41</td>\n",
       "      <td>16590.64</td>\n",
       "      <td>16616.60</td>\n",
       "      <td>34295.37736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-12-29 16:00:00</td>\n",
       "      <td>16616.60</td>\n",
       "      <td>16654.54</td>\n",
       "      <td>16590.82</td>\n",
       "      <td>16612.40</td>\n",
       "      <td>26230.77683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2022-12-29 20:00:00</td>\n",
       "      <td>16612.40</td>\n",
       "      <td>16654.51</td>\n",
       "      <td>16555.57</td>\n",
       "      <td>16633.47</td>\n",
       "      <td>27491.95801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2022-12-30 00:00:00</td>\n",
       "      <td>16633.47</td>\n",
       "      <td>16648.77</td>\n",
       "      <td>16579.75</td>\n",
       "      <td>16599.08</td>\n",
       "      <td>20857.62793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>595</td>\n",
       "      <td>2023-04-07 12:00:00</td>\n",
       "      <td>27840.89</td>\n",
       "      <td>27984.75</td>\n",
       "      <td>27800.00</td>\n",
       "      <td>27944.10</td>\n",
       "      <td>5761.76975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>596</td>\n",
       "      <td>2023-04-07 16:00:00</td>\n",
       "      <td>27944.09</td>\n",
       "      <td>27953.47</td>\n",
       "      <td>27856.00</td>\n",
       "      <td>27897.52</td>\n",
       "      <td>2550.36482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>597</td>\n",
       "      <td>2023-04-07 20:00:00</td>\n",
       "      <td>27897.51</td>\n",
       "      <td>27980.74</td>\n",
       "      <td>27839.02</td>\n",
       "      <td>27906.33</td>\n",
       "      <td>2774.37020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>598</td>\n",
       "      <td>2023-04-08 00:00:00</td>\n",
       "      <td>27906.34</td>\n",
       "      <td>28059.39</td>\n",
       "      <td>27859.02</td>\n",
       "      <td>28023.39</td>\n",
       "      <td>3614.73585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>599</td>\n",
       "      <td>2023-04-08 04:00:00</td>\n",
       "      <td>28023.39</td>\n",
       "      <td>28040.72</td>\n",
       "      <td>27974.14</td>\n",
       "      <td>27979.04</td>\n",
       "      <td>659.52669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                 time      open      high       low     close  \\\n",
       "0             0  2022-12-29 08:00:00  16556.37  16630.00  16517.54  16598.70   \n",
       "1             1  2022-12-29 12:00:00  16598.70  16664.41  16590.64  16616.60   \n",
       "2             2  2022-12-29 16:00:00  16616.60  16654.54  16590.82  16612.40   \n",
       "3             3  2022-12-29 20:00:00  16612.40  16654.51  16555.57  16633.47   \n",
       "4             4  2022-12-30 00:00:00  16633.47  16648.77  16579.75  16599.08   \n",
       "..          ...                  ...       ...       ...       ...       ...   \n",
       "595         595  2023-04-07 12:00:00  27840.89  27984.75  27800.00  27944.10   \n",
       "596         596  2023-04-07 16:00:00  27944.09  27953.47  27856.00  27897.52   \n",
       "597         597  2023-04-07 20:00:00  27897.51  27980.74  27839.02  27906.33   \n",
       "598         598  2023-04-08 00:00:00  27906.34  28059.39  27859.02  28023.39   \n",
       "599         599  2023-04-08 04:00:00  28023.39  28040.72  27974.14  27979.04   \n",
       "\n",
       "          volume  \n",
       "0    28212.71787  \n",
       "1    34295.37736  \n",
       "2    26230.77683  \n",
       "3    27491.95801  \n",
       "4    20857.62793  \n",
       "..           ...  \n",
       "595   5761.76975  \n",
       "596   2550.36482  \n",
       "597   2774.37020  \n",
       "598   3614.73585  \n",
       "599    659.52669  \n",
       "\n",
       "[600 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input and hidden tensors are not at the same device, found input tensor at cuda:0 and hidden tensor at cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m a\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfrom_numpy(a)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     17\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 18\u001b[0m output \u001b[39m=\u001b[39m modelo(a)\n\u001b[0;32m     19\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, t)\n\u001b[0;32m     20\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\56945\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\56945\\Pit\\foftorch\\modelo.py:24\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input_seq)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_seq):\n\u001b[0;32m     22\u001b[0m     \u001b[39m#detach hidden state from history\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_cell \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_cell[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdetach(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_cell[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdetach())\n\u001b[1;32m---> 24\u001b[0m     lstm_out, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_cell \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(input_seq\u001b[39m.\u001b[39;49mview(\u001b[39mlen\u001b[39;49m(input_seq) ,\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhidden_cell)\n\u001b[0;32m     25\u001b[0m     predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(lstm_out\u001b[39m.\u001b[39mview(\u001b[39mlen\u001b[39m(input_seq), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m     26\u001b[0m     \u001b[39mreturn\u001b[39;00m predictions[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\56945\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\56945\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:812\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    811\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 812\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m    813\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[0;32m    814\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    815\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[0;32m    816\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input and hidden tensors are not at the same device, found input tensor at cuda:0 and hidden tensor at cpu"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "x=pd.read_csv('BTCUSDT.csv')\n",
    "modelo=modelo.to(device)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(modelo.parameters(), lr=0.001)\n",
    "\n",
    "for ep in range(15):\n",
    "    print(ep)\n",
    "    for i in range(len(x)-151):\n",
    "        t=torch.tensor([x['close'][i+110]]).to(device)\n",
    "        a=np.array(x.close[i:i+100],dtype=np.float32)\n",
    "        a=torch.from_numpy(a).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = modelo(a)\n",
    "        loss = criterion(output, t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "while i <600:\n",
    "    with torch.no_grad():\n",
    "        t=torch.tensor([x['close'][i]])\n",
    "        a=np.array(x.close[i-110:i-10],dtype=np.float32)\n",
    "        a=torch.from_numpy(a)\n",
    "        output = modelo(a)\n",
    "        print(output.item(),t.item())\n",
    "        i+=1\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "y=Model()\n",
    "\n",
    "data=torch.ones(10)\n",
    "target=torch.ones(2)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(y.parameters(), lr=0.001)\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    output = y(data)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "prediction = y(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1.], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
